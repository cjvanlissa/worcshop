@inproceedings{guptaChasingCarbonElusive2021,
  title = {Chasing {{Carbon}}: {{The Elusive Environmental Footprint}} of {{Computing}}},
  shorttitle = {Chasing {{Carbon}}},
  booktitle = {2021 {{IEEE International Symposium}} on {{High-Performance Computer Architecture}} ({{HPCA}})},
  author = {Gupta, Udit and Kim, Young Geun and Lee, Sylvia and Tse, Jordan and Lee, Hsien-Hsin S. and Wei, Gu-Yeon and Brooks, David and Wu, Carole-Jean},
  date = {2021-02},
  pages = {854--867},
  issn = {2378-203X},
  doi = {10.1109/HPCA51647.2021.00076},
  url = {https://ieeexplore.ieee.org/abstract/document/9407142?casa_token=SmfUDS_ArFcAAAAA:ozoBV2fTd1Z8OoMfeQW7Tp39VPLOc0XtViakgXbljsVImN5xHWmWz8rY2MfBqhoIknd1s_MDBMDL},
  urldate = {2024-09-15},
  abstract = {Given recent algorithm, software, and hardware innovation, computing has enabled a plethora of new applications. As computing becomes increasingly ubiquitous, however, so does its environmental impact. This paper brings the issue to the attention of computer-systems researchers. Our analysis, built on industry-reported characterization, quantifies the environmental effects of computing in terms of carbon emissions. Broadly, carbon emissions have two sources: operational energy consumption, and hardware manufacturing and infrastructure. Although carbon emissions from the former are decreasing thanks to algorithmic, software, and hardware innovations that boost performance and power efficiency, the overall carbon footprint of computer systems continues to grow. This work quantifies the carbon output of computer systems to show that most emissions related to modern mobile and data-center equipment come from hardware manufacturing and infrastructure. We therefore outline future directions for minimizing the environmental impact of computing systems.},
  eventtitle = {2021 {{IEEE International Symposium}} on {{High-Performance Computer Architecture}} ({{HPCA}})},
  keywords = {Carbon dioxide,carbon footprint,Computer architecture,Data center,energy,Energy consumption,Hardware,mobile,Software,Software algorithms,Technological innovation},
  file = {C\:\\Users\\vanlissa\\Zotero\\storage\\J9TEK2BV\\Gupta et al. - 2021 - Chasing Carbon The Elusive Environmental Footprin.pdf;C\:\\Users\\vanlissa\\Zotero\\storage\\DIPFQPS9\\9407142.html}
}

@thesis{peikertTransparencyOpenScience2023,
  type = {doctoralThesis},
  title = {Towards {{Transparency}} and {{Open Science}}},
  author = {Peikert, Aaron},
  date = {2023-10-17},
  institution = {Humboldt-Universität zu Berlin},
  doi = {10.18452/27056},
  url = {https://edoc.hu-berlin.de/handle/18452/28171},
  urldate = {2024-02-23},
  abstract = {Die Psychologie und andere empirische Wissenschaften befinden sich in einer Krise, da vielen Forschenden bewusst geworden ist, dass viele Erkenntnisse nicht so stark empirisch gestützt sind, wie sie einst glaubten.  Es wurden mehrere Ursachen dieser Krise vorgeschlagen: Missbrauch statistischer Methoden, soziologische Verzerrungen und schwache Theorien.  In dieser Dissertation gehe ich davon aus, dass ungenaue Theorien unvermeidlich sind, diese aber mithilfe von Induktion einer empirischen Prüfung unterzogen werden können.  Anhand von Daten können Theorien ergänzt werden, sodass präzise Vorhersagen möglich sind, die sich mit der Realität vergleichen lassen.  Eine solche Strategie ist jedoch mit Kosten verbunden.  Induktion ist daher zwar notwendig, aber führt zu einem übermäßigen Vertrauen in empirische Befunde.  Um empirische Ergebnisse adäquat zu bewerten, muss diese Verzerrung berücksichtigt werden.  Das Ausmaß der Verzerrung hängt von den Eigenschaften des induktiven Prozesses ab.  Einige induktive Prozesse können vollständig transparent gemacht werden, sodass ihre Verzerrung angemessen berücksichtigt werden kann.  Ich zeige, dass dies bei Induktion der Fall ist, die beliebig mit anderen Daten wiederholt werden kann, was die Bedeutung von computergestützter Reproduzierbarkeit unterstreicht.  Induktion, die die Forschenden und ihr kognitives Modell einbezieht, kann nicht beliebig wiederholt werden; daher kann die Verzerrung durch Induktion nur mit Unsicherheit beurteilt werden.  Ich schlage vor, dass die Verringerung dieser Unsicherheit das Ziel von Präregistrierung sein sollte.  Nachdem ich die Ziele von Reproduzierbarkeit und Präregistrierung unter dem Gesichtspunkt der Transparenz über Induktion präzisiert habe, gebe ich in den wissenschaftlichen Artikeln, die als Teil der Dissertation veröffentlicht wurden, Empfehlungen für die praktische Umsetzung beider Verfahren.},
  langid = {english},
  annotation = {Accepted: 2023-10-17T11:10:35Z},
  file = {C:\Users\vanlissa\Zotero\storage\3XTQNMV5\Peikert - 2023 - Towards Transparency and Open Science.pdf}
}

@article{steegenIncreasingTransparencyMultiverse2016,
  title = {Increasing {{Transparency Through}} a {{Multiverse Analysis}}},
  author = {Steegen, Sara and Tuerlinckx, Francis and Gelman, Andrew and Vanpaemel, Wolf},
  date = {2016-09-01},
  journaltitle = {Perspectives on Psychological Science},
  shortjournal = {Perspect Psychol Sci},
  volume = {11},
  number = {5},
  pages = {702--712},
  publisher = {SAGE Publications Inc},
  issn = {1745-6916},
  doi = {10.1177/1745691616658637},
  url = {https://doi.org/10.1177/1745691616658637},
  urldate = {2021-02-11},
  abstract = {Empirical research inevitably includes constructing a data set by processing raw data into a form ready for statistical analysis. Data processing often involves choices among several reasonable options for excluding, transforming, and coding data. We suggest that instead of performing only one analysis, researchers could perform a multiverse analysis, which involves performing all analyses across the whole set of alternatively processed data sets corresponding to a large set of reasonable scenarios. Using an example focusing on the effect of fertility on religiosity and political attitudes, we show that analyzing a single data set can be misleading and propose a multiverse analysis as an alternative practice. A multiverse analysis offers an idea of how much the conclusions change because of arbitrary choices in data construction and gives pointers as to which choices are most consequential in the fragility of the result.},
  langid = {english},
  keywords = {arbitrary choices,data processing,good research practices,multiverse analysis,selective reporting,transparency},
  file = {C:\Users\vanlissa\Zotero\storage\M7M52Q84\Steegen et al_2016_Increasing Transparency Through a Multiverse Analysis.pdf}
}

@article{sterkenburgNofreelunchTheoremsSupervised2021,
  title = {The No-Free-Lunch Theorems of Supervised Learning},
  author = {Sterkenburg, Tom F. and Grünwald, Peter D.},
  date = {2021-12},
  journaltitle = {Synthese},
  shortjournal = {Synthese},
  volume = {199},
  number = {3-4},
  pages = {9979--10015},
  issn = {0039-7857, 1573-0964},
  doi = {10.1007/s11229-021-03233-1},
  url = {https://link.springer.com/10.1007/s11229-021-03233-1},
  urldate = {2023-11-10},
  abstract = {The no-free-lunch theorems promote a skeptical conclusion that all possible machine learning algorithms equally lack justification. But how could this leave room for a learning theory, that shows that some algorithms are better than others? Drawing parallels to the philosophy of induction, we point out that the no-free-lunch results presuppose a conception of learning algorithms as purely data-driven. On this conception, every algorithm must have an inherent inductive bias, that wants justification. We argue that many standard learning algorithms should rather be understood as modeldependent: in each application they also require for input a model, representing a bias. Generic algorithms themselves, they can be given a model-relative justification.},
  langid = {english},
  file = {C:\Users\vanlissa\Zotero\storage\MGXQ9E8N\Sterkenburg and Grünwald - 2021 - The no-free-lunch theorems of supervised learning.pdf}
}

@article{vanlissaWORCSWorkflowOpen2021,
  title = {{{WORCS}}: {{A}} Workflow for Open Reproducible Code in Science},
  author = {Van Lissa, Caspar J. and Brandmaier, Andreas M. and Brinkman, Loek and Lamprecht, Anna-Lena and Peikert, Aaron and Struiksma, Marijn E. and Vreede, Barbara M.I.},
  date = {2021},
  journaltitle = {Data Science},
  volume = {4},
  number = {1},
  pages = {29--49},
  publisher = {IOS Press},
  issn = {2451-8492},
  doi = {10.3233/DS-210031},
  abstract = {Adopting open science principles can be challenging, requiring conceptual education and training in the use of new tools. This paper introduces the Workflow for Open Reproducible Code in Science (WORCS): A step-by-step procedure that researchers can follow to make a research project open and reproducible. This workflow intends to lower the threshold for adoption of open science principles. It is based on established best practices, and can be used either in parallel to, or in absence of, top-down requirements by journals, institutions, and funding bodies. To facilitate widespread adoption, the WORCS principles have been implemented in the R package worcs , which offers an RStudio project template and utility functions for specific workflow steps. This paper introduces the conceptual workflow, discusses how it meets different standards for open science, and addresses the functionality provided by the R implementation, worcs . This paper is primarily targeted towards scholars conducting research projects in R, conducting research that involves academic prose, analysis code, and tabular data. However, the workflow is flexible enough to accommodate other scenarios, and offers a starting point for customized solutions. The source code for the R package and manuscript, and a list of examplesof WORCS projects , are available at https://github.com/cjvanlissa/worcs .},
  keywords = {dynamic document generation,Open science,r,reproducibility,version control}
}
